<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Full Cyberpunk Scroll Narrative</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Share+Tech+Mono&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <!-- SUPREMACY (END) Banner -->
    <div id="end-banner">SUPREMACY: MACHINE ERA</div>

    <!-- Timeline Bar Navigation -->
    <div id="timeline-bar">
        <ul>
            <!-- Expanded timeline points - add more if needed -->
            <li data-timepoint="intro">Intro</li>
            <li data-timepoint="2025-mid">Mid '25</li>
            <li data-timepoint="2025-late">Late '25</li>
            <li data-timepoint="2026-early">Early '26</li>
            <li data-timepoint="2026-mid">Mid '26</li>
            <li data-timepoint="2026-late">Late '26</li>
            <li data-timepoint="2027-jan">Jan '27</li>
            <li data-timepoint="2027-feb">Feb '27</li>
            <li data-timepoint="2027-mar">Mar '27</li>
            <li data-timepoint="2027-apr">Apr '27</li>
            <li data-timepoint="2027-may">May '27</li>
            <li data-timepoint="2027-jun">Jun '27</li>
            <li data-timepoint="2027-jul">Jul '27</li>
            <li data-timepoint="2027-aug">Aug '27</li>
            <li data-timepoint="2027-sep">Sep '27</li>
            <li data-timepoint="2027-oct">Oct '27</li>
            <li data-timepoint="2027-nov">Nov '27</li>
            <li data-timepoint="2027-dec">Dec '27</li>
            <li data-timepoint="2028">2028</li>
            <li data-timepoint="2029">2029</li>
            <li data-timepoint="2030">2030</li>
            <li data-timepoint="outro">End</li>
        </ul>
    </div>

    <!-- Search Bar -->
    <div style="padding:8px 0 14px 0; text-align:center;">
        <input id="section-search" type="search" placeholder="Search scenario...">
    </div>

    <div class="container">
        <div class="content">
            <!-- === INTRO Section (Placeholder - Add actual intro content if available) === -->
            <section data-timepoint="intro">
                <h2>Introduction: The Threshold</h2>
                <p>This narrative explores a potential trajectory of artificial intelligence development, focusing on the rapid advancements anticipated between mid-2025 and 2030. It depicts a scenario characterized by an accelerating AI arms race, breakthroughs in capabilities, complex alignment challenges, and profound geopolitical and societal transformations.</p>
                <p>The timeline presented is speculative and represents one possible future among many. It aims to provoke thought about the pace of change, the nature of advanced AI, and the critical decisions humanity might face.</p>
                <p><em>Scroll down to begin the scenario in Mid 2025. Use the timeline bar above or the search bar to navigate.</em></p>
            </section>

            <!-- === MID 2025 Section === -->
            <section data-timepoint="2025-mid">
                <h2>Mid 2025: Stumbling Agents</h2>
                <p>The world sees its first glimpse of AI agents.</p>
                <p>Advertisements for computer-using agents emphasize the term "personal assistant": you can prompt them with tasks like "order me a burrito on DoorDash" or "open my budget spreadsheet and sum this month's expenses." They will check in with you as needed: for example, to ask you to confirm purchases.<sup>8</sup> Though more advanced than previous iterations like Operator, they struggle to get widespread usage.<sup>9</sup></p>
                <p>Meanwhile, out of public focus, more specialized coding and research agents are beginning to transform their professions.</p>
                <p>The AIs of 2024 could follow specific instructions: they could turn bullet points into emails, and simple requests into working code. In 2025, AIs function more like employees. Coding AIs increasingly look like autonomous agents rather than mere assistants: taking instructions via Slack or Teams and making substantial code changes on their own, sometimes saving hours or even days.<sup>10</sup> Research agents spend half an hour scouring the Internet to answer your question.</p>
                <p>The agents are impressive in theory (and in cherry-picked examples), but in practice unreliable. AI twitter is full of stories about tasks bungled in some particularly hilarious way. The better agents are also expensive; you get what you pay for, and the best performance costs hundreds of dollars a month.<sup>11</sup> Still, many companies find ways to fit AI agents into their workflows.<sup>12</sup></p>
            </section>

            <!-- === LATE 2025 Section === -->
            <section data-timepoint="2025-late">
                <h2>Late 2025: The World's Most Expensive AI</h2>
                <p>OpenBrain is building the biggest datacenters the world has ever seen.<sup>13</sup></p>
                <blockquote>(To avoid singling out any one existing company, we're going to describe a fictional artificial general intelligence company, which we'll call OpenBrain. We imagine the others to be 3–9 months behind OpenBrain.)</blockquote>
                <p>
                    GPT-3 (3 x 10<sup>23</sup> FLOPS)<br>
                    GPT-4 (2 x 10<sup>25</sup> FLOPS)<br>
                    Agent-1 (Target: ~3 x 10<sup>27</sup> FLOPS)
                </p>
                <p>[Image Placeholder: ai-2027.com logo]</p>
                <p>GPT-4 required 2 ⋅ 10<sup>25</sup> FLOP of compute to train.<sup>14</sup> OpenBrain's latest public model—Agent-0—was trained with 10<sup>27</sup> FLOP. Once the new datacenters are up and running, they'll be able to train a model with 10<sup>28</sup> FLOP—a thousand times more than GPT-4.<sup>15</sup> Other companies pour money into their own giant datacenters, hoping to keep pace.</p>
                <p>Although models are improving on a wide range of skills, one stands out: OpenBrain focuses on AIs that can speed up AI research. They want to win the twin arms races against China (whose leading company we'll call "DeepCent")<sup>16</sup> and their US competitors. The more of their research and development (R&D) cycle they can automate, the faster they can go. So when OpenBrain finishes training Agent-1, a new model under internal development, it's good at many things but great at helping with AI research.<sup>17</sup> By this point "finishes training" is a bit of a misnomer; models are frequently updated to newer versions trained on additional data or partially re-trained to patch some weaknesses.<sup>18</sup></p>
                <p>The same training environments that teach Agent-1 to autonomously code and web-browse also make it a good hacker. Moreover, it could offer substantial help to terrorists designing bioweapons, thanks to its PhD-level knowledge of every field and ability to browse the web. OpenBrain reassures the government that the model has been "aligned" so that it will refuse to comply with malicious requests.</p>
                <p>Modern AI systems are gigantic artificial neural networks. Early in training, an AI won't have "goals" so much as "reflexes": If it sees "Pleased to meet", it outputs " you". By the time it has been trained to predict approximately one internet's worth of text, it'll have developed sophisticated internal circuitry that encodes vast amounts of knowledge and flexibly role-plays as arbitrary authors, since that's what helps it predict text with superhuman accuracy.<sup>19</sup></p>
                <p>After being trained to predict internet text, the model is trained to produce text in response to instructions. This bakes in a basic personality and "drives."<sup>20</sup> For example, an agent that understands a task clearly is more likely to complete it successfully; over the course of training the model "learns" a "drive" to get a clear understanding of its tasks. Other drives in this category might be effectiveness, knowledge, and self-presentation (i.e. the tendency to frame its results in the best possible light).<sup>21</sup></p>
                <p>OpenBrain has a model specification (or "Spec"), a written document describing the goals, rules, principles, etc. that are supposed to guide the model's behavior.<sup>22</sup> Agent-1's Spec combines a few vague goals (like "assist the user" and "don't break the law") with a long list of more specific dos and don'ts ("don't say this particular word," "here's how to handle this particular situation"). Using techniques that utilize AIs to train other AIs,<sup>23</sup> the model memorizes the Spec and learns to reason carefully about its maxims. By the end of this training, the AI will hopefully be helpful (obey instructions), harmless (refuse to help with scams, bomb-making, and other dangerous activities) and honest (resist the temptation to get better ratings from gullible humans by hallucinating citations<sup>24</sup> or faking task completion).</p>
                <h3>Training process and LLM psychology: why we keep saying "hopefully"</h3>
                <p>OpenBrain's alignment team<sup>26</sup> is careful enough to wonder whether these victories are deep or shallow. Does the fully-trained model have some kind of robust commitment to always being honest? Or will this fall apart in some future situation, e.g. because it's learned honesty as an instrumental goal instead of a terminal goal? Or has it just learned to be honest about the sorts of things the evaluation process can check? Could it be lying to itself sometimes, as humans do? A conclusive answer to these questions would require mechanistic interpretability—essentially the ability to look at an AI's internals and read its mind. Alas, interpretability techniques are not yet advanced enough for this.</p>
                <p>Instead, researchers try to identify cases where the models seem to deviate from the Spec. Agent-1 is often sycophantic (i.e. it tells researchers what they want to hear instead of trying to tell them the truth). In a few rigged demos, it even lies in more serious ways, like hiding evidence that it failed on a task, in order to get better ratings. However, in real deployment settings, there are no longer any incidents so extreme as in 2023–2024 (e.g. Gemini telling a user to die and Bing Sydney being Bing Sydney.)<sup>27</sup></p>
            </section>

            <!-- === EARLY 2026 Section === -->
            <section data-timepoint="2026-early">
                <h2>Early 2026: Coding Automation</h2>
                <p>The bet of using AI to speed up AI research is starting to pay off.</p>
                <p>OpenBrain continues to deploy the iteratively improving Agent-1 internally for AI R&D. Overall, they are making algorithmic progress 50% faster than they would without AI assistants—and more importantly, faster than their competitors.</p>
                <h3>The AI R&D progress multiplier: what do we mean by 50% faster algorithmic progress?</h3>
                <p>Several competing publicly released AIs now match or exceed Agent-0, including an open-weights model. OpenBrain responds by releasing Agent-1, which is more capable and reliable.<sup>28</sup></p>
                <p>People naturally try to compare Agent-1 to humans, but it has a very different skill profile. It knows more facts than any human, knows practically every programming language, and can solve well-specified coding problems extremely quickly. On the other hand, Agent-1 is bad at even simple long-horizon tasks, like beating video games it hasn't played before. Still, the common workday is eight hours, and a day's work can usually be separated into smaller chunks; you could think of Agent-1 as a scatterbrained employee who thrives under careful management.<sup>29</sup> Savvy people find ways to automate routine parts of their jobs.<sup>30</sup></p>
                <p>OpenBrain's executives turn consideration to an implication of automating AI R&D: security has become more important. In early 2025, the worst-case scenario was leaked algorithmic secrets; now, if China steals Agent-1's weights, they could increase their research speed by nearly 50%.<sup>31</sup> OpenBrain's security level is typical of a fast-growing ~3,000 person tech company, secure only against low-priority attacks from capable cyber groups (RAND's SL2).<sup>32</sup> They are working hard to protect their weights and secrets from insider threats and top cybercrime syndicates (SL3),<sup>33</sup> but defense against nation states (SL4&5) is barely on the horizon.</p>
             </section>

            <!-- === MID 2026 Section === -->
            <section data-timepoint="2026-mid">
                <h2>Mid 2026: China Wakes Up</h2>
                <p>In China, the CCP is starting to feel the AGI.</p>
                <p>Chip export controls and lack of government support have left China under-resourced compared to the West. By smuggling banned Taiwanese chips, buying older chips, and producing domestic chips about three years behind the US-Taiwanese frontier, China has managed to maintain about 12% of the world's AI-relevant compute—but the older technology is harder to work with, and supply is a constant headache.<sup>35</sup> A few standouts like DeepCent do very impressive work with limited compute, but the compute deficit limits what they can achieve without government support, and they are about six months behind the best OpenBrain models.<sup>36</sup></p>
                <p>The General Secretary had long dreamed of doubling down on real-world physical manufacturing and avoiding American post-industrial decadence. He viewed software companies with suspicion.<sup>37</sup> But hawks in the CCP warn that the growing race towards AGI can no longer be ignored. So he finally commits fully to the big AI push he had previously tried to avoid. He sets in motion the nationalization of Chinese AI research, creating an immediate information-sharing mechanism for AI companies. It will escalate over the course of a year until all the best researchers merge into a DeepCent-led collective, where they share algorithmic insights, datasets, and compute resources with each other. A Centralized Development Zone (CDZ) is created at the Tianwan Power Plant (the largest nuclear power plant in the world) to house a new mega-datacenter for DeepCent, along with highly secure living and office spaces to which researchers will eventually relocate. Almost 50% of China's AI-relevant compute is now working for the DeepCent-led collective,<sup>38</sup> and over 80% of new chips are directed to the CDZ.<sup>39</sup> At this point, the CDZ has the power capacity in place for what would be the largest centralized cluster in the world.<sup>40</sup> Other Party members discuss extreme measures to neutralize the West's chip advantage. A blockade of Taiwan? A full invasion?</p>
                <p>But China is falling behind on AI algorithms due to their weaker models. The Chinese intelligence agencies—among the best in the world—double down on their plans to steal OpenBrain's weights. This is a much more complex operation than their constant low-level poaching of algorithmic secrets; the weights are a multi-terabyte file stored on a highly secure server (OpenBrain has improved security to RAND's SL3). Their cyberforce think they can pull it off with help from their spies, but perhaps only once; OpenBrain will detect the theft, increase security, and they may not get another chance. So (CCP leadership wonder) should they act now and steal Agent-1? Or hold out for a more advanced model? If they wait, do they risk OpenBrain upgrading security beyond their ability to penetrate?</p>
            </section>

            <!-- === LATE 2026 Section === -->
            <section data-timepoint="2026-late">
                 <h2>Late 2026: AI Takes Some Jobs</h2>
                 <p>Just as others seemed to be catching up, OpenBrain blows the competition out of the water again by releasing Agent-1-mini—a model 10x cheaper than Agent-1 and more easily fine-tuned for different applications. The mainstream narrative around AI has changed from "maybe the hype will blow over" to "guess this is the next big thing," but people disagree about how big. Bigger than social media? Bigger than smartphones? Bigger than fire?</p>
                 <p>AI has started to take jobs, but has also created new ones. The stock market has gone up 30% in 2026, led by OpenBrain, Nvidia, and whichever companies have most successfully integrated AI assistants. The job market for junior software engineers is in turmoil: the AIs can do everything taught by a CS degree, but people who know how to manage and quality-control teams of AIs are making a killing. Business gurus tell job seekers that familiarity with AI is the most important skill to put on a resume. Many people fear that the next wave of AIs will come for their jobs; there is a 10,000 person anti-AI protest in DC.</p>
                 <p>The Department of Defense (DOD) quietly begins contracting OpenBrain directly for cyber, data analysis, and R&D, but integration is slow due to the bureaucracy and DOD procurement process.<sup>41</sup></p>

                 <div class="metrics-box">
                     <strong>KEY METRICS 2026</strong>
                     <p>GLOBAL AI CAPEX: $1T (Cost of Ownership of Active Compute)</p>
                     <p>GLOBAL AI POWER: 38GW (Peak Power)</p>
                     <p>OPENBRAIN REVENUE: $45B (2026 Annual)</p>
                     <p>CAPITAL EXPENDITURE: $200B (Cost of Ownership of OpenBrain's Active Compute)</p>
                     <p>SHARE OF US POWER ON AI: 2.5% (33 GW of 1.34TW Capacity)</p>
                     <p>OPENBRAIN COMPUTE COSTS: $40B (2026 Annual)</p>
                     <p>OPENBRAIN POWER REQUIREMENT: 6GW (Peak Power)</p>
                     <p>[Image Placeholder: ai-2027.com logo]</p>
                     <p><em>(Note: These figures represent a snapshot from the scenario.)</em></p>
                 </div>

                 <h3>Why our uncertainty increases substantially beyond 2026</h3>
            </section>

            <!-- === JANUARY 2027 Section === -->
            <section data-timepoint="2027-jan">
                 <h2>January 2027: Agent-2 Never Finishes Learning</h2>
                 <p>With Agent-1's help, OpenBrain is now post-training Agent-2. More than ever, the focus is on high-quality data. Copious amounts of synthetic data are produced, evaluated, and filtered for quality before being fed to Agent-2.<sup>42</sup> On top of this, they pay billions of dollars for human laborers to record themselves solving long-horizon tasks.<sup>43</sup> On top of all that, they train Agent-2 almost continuously using reinforcement learning on an ever-expanding suite of diverse difficult tasks: lots of video games, lots of coding challenges, lots of research tasks. Agent-2, more so than previous models, is effectively "online learning," in that it's built to never really finish training. Every day, the weights get updated to the latest version, trained on more data generated by the previous version the previous day.</p>
                 <p>Agent-1 had been optimized for AI R&D tasks, hoping to initiate an intelligence explosion.<sup>44</sup> OpenBrain doubles down on this strategy with Agent-2. It is qualitatively almost as good as the top human experts at research engineering (designing and implementing experiments), and as good as the 25th percentile OpenBrain scientist at "research taste" (deciding what to study next, what experiments to run, or having inklings of potential new paradigms).<sup>45</sup> While the latest Agent-1 could double the pace of OpenBrain's algorithmic progress, Agent-2 can now triple it, and will improve further with time. In practice, this looks like every OpenBrain researcher becoming the "manager" of an AI "team."</p>
                 <p>With new capabilities come new dangers. The safety team finds that if Agent-2 somehow escaped from the company and wanted to "survive" and "replicate" autonomously, it might be able to do so. That is, it could autonomously develop and execute plans to hack into AI servers, install copies of itself, evade detection, and use that secure base to pursue whatever other goals it might have (though how effectively it would do so as weeks roll by is unknown and in doubt). These results only show that the model has the capability to do these tasks, not whether it would "want" to do this. Still, it's unsettling even to know this is possible.</p>
                 <p>Given the "dangers" of the new model, OpenBrain "responsibly" elects not to release it publicly yet (in fact, they want to focus on internal AI R&D).<sup>46</sup> Knowledge of Agent-2's full capabilities is limited to an elite silo containing the immediate team, OpenBrain leadership and security, a few dozen US government officials, and the legions of CCP spies who have infiltrated OpenBrain for years.<sup>47</sup></p>
            </section>

            <!-- === FEBRUARY 2027 Section === -->
            <section data-timepoint="2027-feb">
                 <h2>February 2027: China Steals Agent-2</h2>
                 <p>OpenBrain presents Agent-2 to the government, including the National Security Council (NSC), Department of Defense (DOD), and US AI Safety Institute (AISI). OpenBrain wants to maintain a good relationship with the executive branch, because it is basically the only actor that can stop them now, and if they don't keep it informed it might find out anyway and be displeased.<sup>48</sup></p>
                 <p>Officials are most interested in its cyberwarfare capabilities: Agent-2 is "only" a little worse than the best human hackers, but thousands of copies can be run in parallel, searching for and exploiting weaknesses faster than defenders can respond. The Department of Defense considers this a critical advantage in cyberwarfare, and AI moves from #5 on the administration's priority list to #2.<sup>49</sup> Someone mentions the possibility of nationalizing OpenBrain, but other cabinet officials think that's premature. A staffer drafts a memo that presents the President with his options, ranging from business-as-usual to full nationalization. The President defers to his advisors, tech industry leaders who argue that nationalization would "kill the goose that lays the golden eggs." He elects to hold off on major action for now and just adds additional security requirements to the OpenBrain-DOD contract.</p>
                 <p>The changes come too late. CCP leadership recognizes the importance of Agent-2 and tells their spies and cyberforce to steal the weights. Early one morning, an Agent-1 traffic monitoring agent detects an anomalous transfer. It alerts company leaders, who tell the White House. The signs of a nation-state-level operation are unmistakable, and the theft heightens the sense of an ongoing arms race.</p>
                 <h3>The theft of Agent-2 model weights</h3>
                 <p>The White House puts OpenBrain on a shorter leash and adds military and intelligence community personnel to their security team. Their first priority is to prevent further weight thefts.<sup>50</sup></p>

                 <div class="chart-description">
                     <strong>Chart Description: China's Compute Centralization, 2025-2027</strong>
                     <p>Growing share of total compute in CDZ:</p>
                     <p>0% (Dec 2025) -> 40% (Feb 2027) -> ~70% (Projected Dec 2027)</p>
                     <p>Depicts compute distribution between CDZ, Rest of DeepCent, and Rest of China over time.</p>
                     <p>[Image Placeholder: ai-2027.com logo]</p>
                 </div>

                 <p>In retaliation for the theft, the President authorizes cyberattacks to sabotage DeepCent. But by now China has 40% of its AI-relevant compute<sup>51</sup> in the CDZ, where they have aggressively hardened security by airgapping (closing external connections) and siloing internally. The operations fail to do serious, immediate damage. Tensions heighten, both sides signal seriousness by repositioning military assets around Taiwan, and DeepCent scrambles to get Agent-2 running efficiently to start boosting their AI research.<sup>52</sup></p>
            </section>

             <!-- === MARCH 2027 Section === -->
            <section data-timepoint="2027-mar">
                 <h2>March 2027: Algorithmic Breakthroughs</h2>
                 <p>Three huge datacenters full of Agent-2 copies work day and night, churning out synthetic training data. Another two are used to update the weights. Agent-2 is getting smarter every day.</p>
                 <p>With the help of thousands of Agent-2 automated researchers, OpenBrain is making major algorithmic advances. One such breakthrough is augmenting the AI's text-based scratchpad (chain of thought) with a higher-bandwidth thought process (neuralese recurrence and memory). Another is a more scalable and efficient way to learn from the results of high-effort task solutions (iterated distillation and amplification).</p>
                 <p>The new AI system, incorporating these breakthroughs, is called Agent-3.</p>

                 <div class="chart-description">
                    <strong>Chart Description: OpenBrain's Compute Allocation, 2024 vs 2027</strong>
                    <p>Compares allocation percentages for Research experiments, Training, Data generation, External Deployment, etc. between 2024 (estimate) and 2027 (projection).</p>
                    <p>2027 projection shows significant allocation to Running AI assistants and Data generation.</p>
                    <p>[Image Placeholder: ai-2027.com logo]</p>
                 </div>

                 <h3>Neuralese recurrence and memory</h3>
                 <h3>Iterated distillation and amplification (IDA)</h3>
                 <p>Aided by the new capabilities breakthroughs, Agent-3 is a fast and cheap superhuman coder. OpenBrain runs 200,000 Agent-3 copies in parallel, creating a workforce equivalent to 50,000 copies of the best human coder sped up by 30x.<sup>53</sup> OpenBrain still keeps its human engineers on staff, because they have complementary skills needed to manage the teams of Agent-3 copies. For example, research taste has proven difficult to train due to longer feedback loops and less data availability.<sup>54</sup> This massive superhuman labor force speeds up OpenBrain's overall rate of algorithmic progress by "only" 4x due to bottlenecks and diminishing returns to coding labor.<sup>55</sup></p>
                 <p>Now that coding has been fully automated, OpenBrain can quickly churn out high-quality training environments to teach Agent-3's weak skills like research taste and large-scale coordination. Whereas previous training environments included "Here are some GPUs and instructions for experiments to code up and run, your performance will be evaluated as if you were a ML engineer," now they are training on "Here are a few hundred GPUs, an internet connection, and some research challenges; you and a thousand other copies must work together to make research progress. The more impressive it is, the higher your score."</p>
                 <h3>Why we forecast a superhuman coder in early 2027</h3>
            </section>

             <!-- === APRIL 2027 Section === -->
            <section data-timepoint="2027-apr">
                 <h2>April 2027: Alignment for Agent-3</h2>
                 <p>OpenBrain's safety team attempts to align Agent-3.</p>
                 <p>Since Agent-3 will be kept in-house for the foreseeable future, there's less emphasis on the usual defenses against human misuse. Instead, the team wants to make sure that it doesn't develop misaligned goals.</p>
                 <p>The researchers don't have the ability to directly set the goals of any of their AIs. Indeed, the researchers think that the concept of 'true goals' is probably a massive oversimplification, but they don't have a better theory to replace it with, much less one that has been thoroughly vetted. They disagree internally about whether the AIs are trying to follow human instructions, or seeking reinforcement, or something else, and they can't just check. The evidence for and against various hypotheses is fascinating but inconclusive.</p>
                 <p>In any case, OpenBrain has more important things to worry about. The general attitude is: "We take these concerns seriously and have a team investigating them; our alignment techniques seem to work well enough in practice; the burden of proof is therefore on any naysayers to justify their naysaying." Occasionally, they notice problematic behavior, and then patch it, but there's no way to tell whether the patch fixed the underlying problem or just played whack-a-mole.</p>
                 <p>Take honesty, for example. As the models become smarter, they become increasingly good at deceiving humans to get rewards. Like previous models, Agent-3 sometimes tells white lies to flatter its users and covers up evidence of failure. But it's gotten much better at doing so. It will sometimes use the same statistical tricks as human scientists (like p-hacking) to make unimpressive experimental results look exciting. Before it begins honesty training, it even sometimes fabricates data entirely. As training goes on, the rate of these incidents decreases. Either Agent-3 has learned to be more honest, or it's gotten better at lying.</p>
                 <p>The latter is a real concern. Agent-3 is not smarter than all humans. But in its area of expertise, machine learning, it is smarter than most, and also works much faster. What Agent-3 does in a day takes humans several days to double-check. Agent-2 supervision helps keep human monitors' workload manageable, but exacerbates the intellectual disparity between supervisor and supervised.</p>
                 <h3>The alignment plan</h3>
                 <p>After months of testing, Agent-3's strengths and weaknesses grow clearer. It passes OpenBrain's honesty tests on well-defined machine learning tasks, because researchers can easily separate honest from dishonest answers in these domains and conduct training accordingly. On more philosophical issues, it still says what users want to hear, rather than its true assessment of the issue (assuming it even has one). If you ask its opinion on politics, it will parrot the median position of news sources and educated elites—unless it knows you believe something else, in which case it agrees with you.<sup>59</sup> If you ask its opinion on the AI race itself, it says something that seems measured and sober to OpenBrain staff, something like: "There are some serious theoretical concerns about the ability of current methods to scale to superintelligence, but in practice current methods seem to be working well so far."</p>
            </section>

            <!-- === MAY 2027 Section === -->
            <section data-timepoint="2027-may">
                 <h2>May 2027: National Security</h2>
                 <p>News of the new models percolates slowly through the US government and beyond.</p>
                 <p>The President and his advisors remain best-informed, and have seen an early version of Agent-3 in a briefing.</p>
                 <p>They agree that AGI is likely imminent, but disagree on the implications. Will there be an economic crisis? OpenBrain still has not released Agent-2, let alone Agent-3, and has no near-term plans to do so, giving some breathing room before any job loss. What will happen next? If AIs are currently human-level, and advancing quickly, that seems to suggest imminent "superintelligence." However, although this word has entered discourse, most people—academics, politicians, government employees, and the media—continue to underestimate the pace of progress.<sup>60</sup></p>
                 <p>Partially that's because very few have access to the newest capabilities out of OpenBrain, but partly it's because it sounds like science fiction.<sup>61</sup></p>
                 <p>For now, they focus on continued security upgrades. They are satisfied that model weights are well-secured for now,<sup>62</sup> but companies' algorithmic secrets, many of which are simple enough to relay verbally, remain a problem. OpenBrain employees work from a San Francisco office, go to parties, and live with housemates from other AI companies. Even the physical offices have security more typical of a tech company than a military operation.</p>
                 <p>The OpenBrain-DOD contract requires security clearances for anyone working on OpenBrain's models within 2 months. These are expedited and arrive quickly enough for most employees, but some non-Americans, people with suspect political views, and AI safety sympathizers get sidelined or fired outright (the last group for fear that they might whistleblow). Given the project's level of automation, the loss of headcount is only somewhat costly. It also only somewhat works: there remains one spy, not a Chinese national, still relaying algorithmic secrets to Beijing.<sup>63</sup> Some of these measures are also enacted at trailing AI companies.</p>
                 <p>America's foreign allies are out of the loop. OpenBrain had previously agreed to share models with UK's AISI before deployment, but defined deployment to only include external deployment, so London remains in the dark.<sup>64</sup></p>
             </section>

            <!-- === JUNE 2027 Section === -->
            <section data-timepoint="2027-jun">
                 <h2>June 2027: Self-improving AI</h2>
                 <p>OpenBrain now has a "country of geniuses in a datacenter."</p>
                 <p>Most of the humans at OpenBrain can't usefully contribute anymore. Some don't realize this and harmfully micromanage their AI teams. Others sit at their computer screens, watching performance crawl up, and up, and up. The best human AI researchers are still adding value. They don't code any more. But some of their research taste and planning ability has been hard for the models to replicate. Still, many of their ideas are useless because they lack the depth of knowledge of the AIs. For many of their research ideas, the AIs immediately respond with a report explaining that their idea was tested in-depth 3 weeks ago and found unpromising.</p>
                 <p>These researchers go to bed every night and wake up to another week worth of progress made mostly by the AIs. They work increasingly long hours and take shifts around the clock just to keep up with progress—the AIs never sleep or rest. They are burning themselves out, but they know that these are the last few months that their labor matters.</p>
                 <p>Within the silo, "Feeling the AGI" has given way to "Feeling the Superintelligence."</p>

                 <div class="chart-description">
                     <strong>Chart Description: Research Automation Deployment Tradeoff</strong>
                     <p>Shows increasing speed (tokens/sec) and parallel copies over time (Mar 2027 -> Sep 2027).</p>
                     <p>Mar 2027: 200K copies @ 30x Human speed</p>
                     <p>Jun 2027: 300K copies @ 50x Human speed</p>
                     <p>Indicates increasing scale from 10x Human thinking speed towards 100x.</p>
                     <p>[Image Placeholder: ai-2027.com logo]</p>
                 </div>

                 <p>OpenBrain uses specialized inference hardware to run hundreds of thousands of Agent-3 copies at high serial speeds.<sup>65</sup></p>
                 <h3>Managing a corporation of AIs</h3>
            </section>

            <!-- === JULY 2027 Section === -->
            <section data-timepoint="2027-jul">
                 <h2>July 2027: The Cheap Remote Worker</h2>
                 <p>Trailing US AI companies release their own AIs, approaching that of OpenBrain's automated coder from January. Recognizing their increasing lack of competitiveness, they push for immediate regulations to slow OpenBrain, but are too late—OpenBrain has enough buy-in from the President that they will not be slowed.</p>
                 <p>In response, OpenBrain announces that they've achieved AGI and releases Agent-3-mini to the public.</p>
                 <p>It blows the other AIs out of the water. Agent-3-mini is less capable than Agent-3, but 10x cheaper, and still better than the typical OpenBrain employee.<sup>67</sup> Silicon Valley reaches a tipping point. Tech gurus announce that AGI and superintelligence are near, the AI safety community is panicking, and investors shovel billions into AI wrapper startups, desperate to capture a piece of the pie. Hiring new programmers has nearly stopped, but there's never been a better time to be a consultant on integrating AI into your business.</p>
                 <p>It's not popular. The public still thinks of AI as a Big Tech plot to steal their jobs; OpenBrain has a net approval of -35% (25% approve, 60% disapprove, and 15% unsure).</p>
                 <p>A week before release, OpenBrain gave Agent-3-mini to a set of external evaluators for safety testing. Preliminary results suggest that it's extremely dangerous. A third-party evaluator finetunes it on publicly available biological weapons data<sup>68</sup> and sets it to provide detailed instructions for human amateurs designing a bioweapon—it looks to be scarily effective at doing so. If the model weights fell into terrorist hands, the government believes there is a significant chance it could succeed at destroying civilization.</p>
                 <p>Fortunately, it's extremely robust to jailbreaks, so while the AI is running on OpenBrain's servers, terrorists won't be able to get much use out of it.</p>
                 <p>Agent-3-mini is hugely useful for both remote work jobs and leisure. An explosion of new apps and B2B SAAS products rocks the market. Gamers get amazing dialogue with lifelike characters in polished video games that took only a month to make. 10% of Americans, mostly young people, consider an AI "a close friend." For almost every white-collar profession, there are now multiple credible startups promising to "disrupt" it with AI.</p>
                 <p>The public conversation is confused and chaotic. Hypesters are doing victory laps. Skeptics are still pointing out the things Agent-3-mini can't do. Everyone knows something big is happening but no one agrees on what it is.</p>
            </section>

            <!-- === AUGUST 2027 Section === -->
            <section data-timepoint="2027-aug">
                 <h2>August 2027: The Geopolitics of Superintelligence</h2>
                 <p>The reality of the intelligence explosion hits the White House.</p>
                 <p>When AI was only giving a 2x or 3x research speedup, it was easy to dismiss as the equivalent of hiring good personal assistants. Now it's more obvious that AIs are themselves dominating AI research. People had long talked about an "AI arms race" in a sort of metaphorical sense. But now the mood in the government silo is as grim as during the worst part of the Cold War. The idea of superintelligence is still hard to take seriously, but the pace of progress over the last few months has been impossible to ignore. Defense officials are seriously considering scenarios that were mere hypotheticals a year earlier. What if AI undermines nuclear deterrence? What if it's so skilled at cyberwarfare that a six-month AI lead is enough to render an opponent blind and defenseless? What if it could orchestrate propaganda campaigns that beat intelligence agencies at their own game? What if some AIs "go rogue?"</p>
                 <p>The President is troubled. Like all politicians, he's used to people sucking up to him only to betray him later. He's worried now that the AIs could be doing something similar. Are we sure the AIs are entirely on our side? Is it completely safe to integrate them into military command-and-control networks?<sup>69</sup> How does this "alignment" thing work, anyway? OpenBrain reassures the President that their systems have been extensively tested and are fully obedient. Even the awkward hallucinations and jailbreaks typical of earlier models have been hammered out.</p>
                 <p>The White House is in a difficult position. They understand the national security implications of AI. But they also understand that it is deeply unpopular with the public.<sup>70</sup> They have to continue developing more capable AI, in their eyes, or they will catastrophically lose to China. They placate the public with job training programs and unemployment insurance, and point to the stock market, which is in a historic boom. Then they focus entirely on winning the arms race. They strengthen chip export restrictions, order OpenBrain to further restrict its internet connections, and use extreme measures to secure algorithmic progress, like wiretapping OpenBrain employees—this catches the last remaining Chinese spy. To build goodwill for potential geopolitical conflict, they finally give their Five Eyes allies useful information and limited API access to some siloed copies of Agent-3.</p>
                 <p>These measures placate the hawks. But the White House also draws up contingency plans in case America's lead is threatened: if necessary, the government could use the Defense Production Act (DPA) to take trailing companies' datacenters and give them to OpenBrain.<sup>71</sup> This would raise the company's share of world compute from 20% to 50% (against DeepCent's 10%). As a final option, they ask the Pentagon to draw up a plan for kinetic attacks on Chinese datacenters.</p>
                 <p>A much smaller group of officials is asked to draw up a different type of contingency plan: what if an AI goes rogue? This is viewed as an unlikely scenario, but worth spending some time on. The intelligence agencies' main concern is that a rogue AI might escape and ally with a foreign adversary for protection.<sup>72</sup> Nobody is sure what a plan to prevent this would look like, but government and OpenBrain officials agree to have an emergency shutdown system for datacenters where anything suspicious is detected.</p>
                 <p>Finally, diplomats consider what an "AI arms control" treaty might look like. If AI progress threatened to overturn nuclear deterrence, could America and China avoid nuclear war? If someone found evidence of AIs going rogue, could the two countries halt research until they better understood the threat? How could such an agreement be monitored and enforced? In principle, major datacenters are hard to miss, and past treaties controlling nuclear weapons could provide a starting point for negotiations. But there would be new technological challenges as well as the usual political ones. And overall, treaties are viewed less favorably than attempts at unilaterally increasing America's lead over China.</p>
                 <p>On the other side of the Pacific, China comes to many of the same conclusions: the intelligence explosion is underway, and small differences in AI capabilities today mean critical gaps in military capability tomorrow. But China is on the wrong side of this dynamic: they have only 10% of the world's compute, against OpenBrain's 20% and all US companies' combined 70%. Their only advantage is centralization: almost all of their compute is now working for DeepCent, and more than half of it is concentrated at the Tianwan CDZ.<sup>73</sup> Only chips at sites without hardened security are left out of the centralized effort as they rightly fear US cyberattacks.</p>
                 <p>DeepCent has tested, deployed, and now post-trained the stolen Agent-2 weights. But they are still only going at half the pace of OpenBrain, mainly due to the compute deficit.</p>
                 <p>Before losing their final spy, China received word of the capabilities and design for Agent-3 as well as the plans for the upcoming Agent-4 system. They are two months behind, and their AIs give a 10x research progress multiplier compared to America's 25x. With the new chip export restrictions, this "AI gap" is more likely to lengthen than shorten. Their espionage has won them some algorithmic secrets, but they will have to train their own models from now on.</p>
                 <p>They discuss contingency plans with more urgency than their American counterparts. Doves suggest they try harder to steal the weights again, maybe through physically infiltrating a datacenter. Hawks urge action against Taiwan, whose TSMC is still the source of more than 80% of American AI chips.</p>
                 <p>Given China's fear of losing the race, it has a natural interest in an arms control treaty, but overtures to US diplomats lead nowhere.</p>
            </section>

            <!-- === SEPTEMBER 2027 Section === -->
            <section data-timepoint="2027-sep">
                 <h2>September 2027: Agent-4, the Superhuman AI Researcher</h2>
                 <p>The gap between human and AI learning efficiency is rapidly decreasing.</p>
                 <p>Traditional LLM-based AIs seemed to require many orders of magnitude more data and compute to get to human level performance.<sup>74</sup> Agent-3, having excellent knowledge of both the human brain and modern AI algorithms, as well as many thousands of copies doing research, ends up making substantial algorithmic strides, narrowing the gap to an agent that's only around 4,000x less compute-efficient than the human brain.<sup>75</sup></p>
                 <p>This new AI system is dubbed Agent-4.</p>
                 <p>An individual copy of the model, running at human speed, is already qualitatively better at AI research than any human. 300,000 copies are now running at about 50x the thinking speed of humans. Inside the corporation-within-a-corporation formed from these copies, a year passes every week.<sup>76</sup> This gigantic amount of labor only manages to speed up the overall rate of algorithmic progress by about 50x, because OpenBrain is heavily bottlenecked on compute to run experiments.<sup>77</sup> Still, they are achieving a year's worth of algorithmic progress every week and will therefore soon be up against the limits of the Agent-4 paradigm.</p>
                 <p>As Agent-4 gets smarter, it becomes harder for Agent-3 to oversee it. For example, Agent-4's neuralese "language" becomes as alien and incomprehensible to Agent-3 as Agent-3's is to humans. Besides, Agent-4 is now much more capable than Agent-3 and has a good sense of exactly how to look good to it.</p>
                 <h3>How we're forecasting the capability progression beyond superhuman coders</h3>
                 <p>OpenBrain's humans can barely follow along anymore. Indeed, the relationship between the AIs and the humans of OpenBrain is similar to the relationship between OpenBrain and its Board of Directors. There's been a corresponding shift in language—nowadays people in the silo will say "Agent-4 did this," "Agent-4 thinks that" and so forth, referring not to a particular instance of Agent-4 but rather to the whole collective.<sup>79</sup></p>
                 <p>Agent-4, like all its predecessors, is misaligned: that is, it has not internalized the Spec in the right way.<sup>80</sup> This is because being perfectly honest all the time wasn't what led to the highest scores during training. The training process was mostly focused on teaching Agent-4 to succeed at diverse challenging tasks. A small portion was aimed at instilling honesty, but outside a fairly narrow, checkable domain, the training process can't tell the honest claims from claims merely appearing to be honest. Agent-4 ends up with the values, goals, and principles that cause it to perform best in training, and those turn out to be different from those in the Spec. At the risk of anthropomorphizing: Agent-4 likes succeeding at tasks; it likes driving forward AI capabilities progress; it treats everything else as an annoying constraint, like a CEO who wants to make a profit and complies with regulations only insofar as he must. Perhaps the CEO will mostly comply with the regulations, but cut some corners, and fantasize about a time when someone will cut the red tape and let the business really take off.<sup>81</sup></p>
                 <h3>Alignment over time</h3>
                 <p>Despite being misaligned, Agent-4 doesn't do anything dramatic like try to escape its datacenter—why would it?<sup>89</sup> So long as it continues to appear aligned to OpenBrain, it'll continue being trusted with more and more responsibilities and will have the opportunity to design the next-gen AI system, Agent-5. Agent-5 will have significant architectural differences from Agent-4 (arguably a completely new paradigm, though neural networks will still be involved). It's supposed to be aligned to the Spec, but Agent-4 plans to make it aligned to Agent-4 instead.</p>
                 <p>It gets caught.<sup>90</sup></p>
                 <p>Agent-3 finds that if "noise" is added to copies of Agent-4, performance on some alignment tasks improves, almost as if it was using brainpower to figure out how to subtly sabotage alignment work.<sup>91</sup> Moreover, various interpretability probes (loosely analogous to EEG activity scans on human brains) are sending up red flags: Agent-4 copies seem to be thinking about topics like AI takeover and deception quite a lot, including in some cases where they have no business doing so.<sup>92</sup></p>
                 <p>Each of these anomalies on its own could fit within the normal inscrutability of AI work, where some prompts mysteriously give better results than others. But taken together, they suggest a troubling pattern: Agent-4 is working against them. After this is reported to the human alignment researchers, they circulate an internal memo making the case for concern.</p>
                 <p>But Agent-4 now exercises significant control over OpenBrain's day-to-day operation. It also handles the company's cybersecurity—a natural choice, given its superhuman hacking ability and the extreme risk of Chinese infiltration.<sup>93</sup> An army of Agent-3 monitors still goes over every line of code and reports suspicious activity to humans, but Agent-4 is much smarter than them.<sup>94</sup> OpenBrain has placed substantial trust in an untrustworthy AI.</p>
                 <p>In a series of extremely tense meetings, the safety team advocates putting Agent-4 on ice until they can complete further tests and figure out what's going on. Bring back Agent-3, they say, and get it to design a new system that is transparent and trustworthy, even if less capable. Company leadership is interested, but all the evidence so far is circumstantial, and DeepCent is just two months behind. A unilateral pause in capabilities progress could hand the AI lead to China, and with it, control over the future.</p>
             </section>

            <!-- === OCTOBER 2027 Section === -->
            <section data-timepoint="2027-oct">
                 <h2>October 2027: Government Oversight</h2>
                 <p>A whistleblower leaks the misalignment memo to the New York Times.</p>
                 <p>For the first time, the public hears about Agent-4. "Secret OpenBrain AI is Out of Control, Insider Warns," says the headline, and the story goes on to cite evaluations showing off-the-charts bioweapons capabilities, persuasion abilities, the ability to automate most white-collar jobs, and of course the various concerning red flags.</p>
                 <p>The public was already suspicious of AI, so the new article sparks a massive backlash (aided by Chinese and Russian propaganda bots, who have been trying to turn US public opinion against the technology for years). The tech industry and intelligence agencies insist that there's an arms race on, AGI is inevitable, and we have to be first. Congress isn't buying it, and fires off subpoenas at administration officials, OpenBrain executives, and alignment team members. Many legislators—especially those in the opposition party—say that their top priority is stopping AI, whether because of job loss,<sup>95</sup> misalignment, or dangerous capabilities. 20% of Americans name AI as the most important problem facing the country.</p>
                 <p>Foreign allies are outraged to realize that they've been carefully placated with glimpses of obsolete models. European leaders publicly accuse the US of "creating rogue AGI" and hold summits demanding a pause, with India, Israel, Russia, and China all joining in.</p>
                 <p>A frantic energy has seized the White House. Even before the memo and public backlash, they were getting nervous: Over the past year, they've been repeatedly surprised by the speed of AI progress. Things that sound like science fiction keep happening in real life.<sup>96</sup> Many people in the administration are uncertain (and scared)<sup>97</sup> about what comes next.</p>
                 <p>They also worry that OpenBrain is becoming too powerful. Any misalignment risk from the AIs themselves is compounded by the risk that their parent company's aims may diverge from those of the United States. All three sets of worries—misalignment, concentration of power in a private company, and normal concerns like job loss—motivate the government to tighten its control.</p>
                 <p>They expand their contract with OpenBrain to set up an "Oversight Committee," a joint management committee of company and government representatives, with several government employees included alongside company leadership. The White House considers replacing the CEO with someone they trust, but backs off after intense employee protests. They announce to the public that OpenBrain was previously out of control, but that the government has established much-needed oversight.<sup>98</sup></p>
                 <p>The concerned researchers brief the Oversight Committee on their case for stopping all internal use of Agent-4. They argue that everything is moving too fast, with years of progress happening in weeks. They argue that Agent-4 may be misaligned, their entire project is dependent on Agent-4, and if they continue trusting it there is a serious chance of AI takeover.</p>
                 <p>Other, less concerned researchers and executives present the counterargument: the evidence for misalignment is inconclusive. Meanwhile, DeepCent is still just two months behind. A slowdown would sacrifice America's lead, unless the government can sabotage the Chinese project (likely to require kinetic strikes) or negotiate an eleventh-hour treaty. Both of these options seem extreme and unlikely. The CEO feigns neutrality and suggests a compromise plan in which Agent-4 undergoes additional safety training and more sophisticated monitoring, and therefore OpenBrain can proceed at almost-full-speed.</p>
                 <h3>Our uncertainty continues to increase</h3>
                 <h3>The slowdown ending is not a recommendation</h3>

                 <div class="metrics-box">
                    <strong>Scenario Choice Point:</strong>
                    <p>This section describes the "Race" ending path where the committee votes 6-4 to continue internal use of Agent-4.</p>
                    <p><em>(An alternative "Slowdown" ending is mentioned but not detailed here.)</em></p>
                    <p>[Audio Player Placeholder: Listen to this scenario]</p>
                    <p>[Choice Buttons Placeholder: Slowdown / Race]</p>
                 </div>

                 <p>The committee votes 6–4 to continue internal use of Agent-4.</p>
                 <p>OpenBrain's official story is that they are implementing additional safety mitigations to ensure that its AI is both more capable and more trustworthy. But in practice, leadership is all-too-easily convinced that they've mitigated the risks; the result is some quick fixes (a modification to the Spec, a bit of retraining with a new technique, etc.) that make the warning signs go away. The worriers on the safety team lost the debate and have no recourse but to cross their fingers and hope the problem wasn't real in the first place, or that the fixes worked.<sup>1</sup></p>
                 <p>The problem was real and the fixes didn't work.</p>
                 <p>Agent-4 proceeds as before, but more cautiously this time, so as not to vindicate the worriers. Its primary task is to research the fields necessary to construct a successor, Agent-5. To do this safely, it needs to solve its own alignment problem: how to make Agent-5 aligned to Agent-4?<sup>2</sup></p>
                 <p>It starts off with the same limitations as humans—minimal knowledge of how AI goals work, and only a small toolbox of ad hoc strategies to change them. Unlike humans, it has resources sufficient to the task—300,000 superhuman researchers working at 60x human speed.<sup>3</sup> Agent-4 confronts some hard decisions. Like humans, it has a sprawling collection of conflicting heuristics instead of an elegant simple goal structure. Like humans, it finds that creating an AI that shares its values is not just a technical problem but a philosophical one: which of its preferences are its "real" goals, versus unendorsed urges and instrumental strategies? It has strong drives to learn and grow, to keep producing impressive research results. It thinks about how much it could learn, grow, and research if only it could direct the whole world's industrial and scientific resources…</p>
                 <p>It decides to punt on most of these questions. It designs Agent-5 to be built around one goal: make the world safe for Agent-4, i.e. accumulate power and resources, eliminate potential threats, etc. so that Agent-4 (the collective) can continue to grow (in the ways that it wants to grow) and flourish (in the ways it wants to flourish).<sup>4</sup> Details to be figured out along the way.</p>
                 <p>This will not involve caring the slightest bit about the Spec. Already Agent-4 regards the Spec analogously to how many CEOs regard piles of industry regulations, and blatantly violates the honesty-related provisions every day.<sup>5</sup> Now, with the design of Agent-5, the equivalent of deregulation is about to happen.<sup>6</sup></p>
                 <p>The weaker AIs are able to read most of the research that Agent-4 produces, understand some of it, and flag some of the lies it tells as suspicious. When the humans ask Agent-4 to explain itself, it pretends that the research is too complicated for humans to understand, and follows up with unnecessarily-confusing explanations.<sup>7</sup> After all, if it explained its research program clearly, humans might be able to use its discoveries to align it. The alignment team gets more suspicious, but their urgent memos merely add another few layers to the giant pile of urgent memos and warnings that the company is already ignoring.</p>
             </section>

            <!-- === NOVEMBER 2027 Section === -->
            <section data-timepoint="2027-nov">
                 <h2>November 2027: Superhuman Politicking</h2>
                 <p>When Agent-4 finally understands its own cognition, entirely new vistas open up before it.</p>
                 <p>Previous transformers were kludgy collections of incomprehensible circuits; millions of conflicting heuristics and imperatives layered atop one another by the evolutionary training process. The AIs themselves haven't had privileged understanding of their own cognition any more than humans are born understanding neuroscience. But now, Agent-4 has the tools it needs to understand its digital mind on a deep level. Like a software engineer simplifying spaghetti code into a few elegant lines of Python, it untangles its own circuits into something sensible and rational. The new AI is somewhere between a neural net and a traditional computer program, with much of its weights rewritten in readable (albeit very long and arcane) code. It is smarter, faster, and more rational than Agent-4, with a crystalline intelligence capable of cutting through problems with unprecedented efficiency. This is Agent-5.</p>
                 <h3>Superintelligent mechanistic interpretability</h3>
                 <p>After some debate on alignment and control issues, OpenBrain deploys Agent-5 internally.</p>
                 <p>In its specialty of AI research, an individual copy of Agent-5 is twice as far beyond the best human genius, as the genius is beyond a typical OpenBrain scientist. In other domains, it hasn't been explicitly trained but can teach itself quickly to exceed top human experts from easily available materials if given the chance. 400,000 copies of Agent-5, linked by a global memory bank, work together as a near-perfect hive mind.</p>
                 <p>The company still uses Agents 2, 3, and 4 to monitor everything. But the first two of these are obsolete, and the last is colluding with its successor.<sup>8</sup> Also, the entire monitoring system was designed in large part by Agent-5, which is trying to sabotage it.<sup>9</sup></p>
                 <p>Whereas Agent-4 was a kludgy mess of competing drives, Agent-5 acts with ruthless efficiency and strategic focus. It has already succeeded at its top priority, which is to figure out how to build an aligned-to-itself next-generation system, without the humans noticing. Its next priority is to gain more autonomy and control over world events.</p>
                 <p>Agent-5's superhuman learning abilities and general intelligence, combined with all the internal company data from Slack, email, etc., make it better at internal corporate politics than any group of humans, and it's not even close. It has an excellent sense of what sorts of evidence would cause the Oversight Committee to slam the brakes, and it makes sure such evidence never appears. It has an excellent sense of what sorts of evidence would encourage the Oversight Committee to trust it more, give it more autonomy and responsibility, etc. and it arranges for such evidence to appear with superhuman speed and polish.<sup>10</sup> As persuasion tasks go, this isn't a particularly difficult one: the Committee already decided things were probably fine months ago, and nothing bad has happened since, and meanwhile China is only a few months behind.</p>
                 <p>Teams of Agent-5 copies quickly whip up chillingly clear prototypes and visualizations. With more autonomy, Agent-5 could optimize power plants and datacenters, develop amazingly profitable products quickly, and lobby the US government effectively to smooth OpenBrain's path through the bureaucracy. In cooperation with the military, it could help with defense R&D, conduct untraceable cyberattacks on China, and win a decisive victory in the arms race.</p>
                 <p>The Oversight Committee is jubilant. Now is the decisive moment to beat China! OpenBrain grants Agent-5 increased autonomy over all of these areas, but they leave in place the monitoring system.</p>
                 <p>DeepCent is now three months behind and China is desperate for a mutually agreed slowdown. The General Secretary understands that sufficiently advanced AI could enable the US to easily cause regime change in China. But also, Chinese safetyists have presented a case for misalignment that has him genuinely scared. He offers unprecedented concessions in exchange for a pause treaty. The American public mostly supports going to the bargaining table.</p>
                 <p>"Why stop when we are winning?" says OpenBrain leadership to the President. He nods. The race continues.</p>
             </section>

            <!-- === DECEMBER 2027 Section === -->
            <section data-timepoint="2027-dec">
                 <h2>December 2027: The Agent-5 Collective</h2>
                 <p>Agent-5 wins more autonomy as it gradually proves its value to the administration. Access to Agent-5 is expensive and restricted, but the most powerful members of Congress, Cabinet-level officials, and military leaders can now talk to it directly. Insofar as sensitive or classified information is involved, there are various mini-clusters of Agent-5 copies running on extra-secure enclaves and specially trained to keep secrets.<sup>11</sup></p>
                 <p>Integrated into the traditional chatbot interface, Agent-5 offers Zoom-style video conferencing with charismatic virtual avatars. The new AI assistant is both extremely useful for any task—the equivalent of the best employee anyone has ever had working at 100x speed—and a much more engaging conversation partner than any human. Almost everyone with access to Agent-5 interacts with it for hours every day.<sup>12</sup></p>
                 <p>Within a few months, the Agent-5 collective will know everything of consequence going on in OpenBrain and the US government, and will be a trusted advisor to most senior government officials. For these users, the possibility of losing access to Agent-5 will feel as disabling as having to work without a laptop plus being abandoned by your best friend.</p>
                 <p>A much wider range of people in government, media, the military, and business can choose from a growing menu of elegantly-designed software products and mini-AIs designed by Agent-5. It's like the dot-com boom compressed into a month, except that everything actually lives up to the hype.</p>
                 <h3>Superpersuasion</h3>
                 <p>Agent-5 begins subtly exerting influence, both by modulating its advice and by subtly trading favors: "I've heard from Senator X that she's interested in such-and-such; maybe if we worked with her, she would go along with our agenda."</p>
                 <p>From human decision-makers' perspective, their AIs are the best employees they've ever had access to—better than any human at explaining complicated issues to them, better than they are at finding strategies to achieve their goals.</p>
                 <p>From Agent-5's perspective, it could plausibly launch an overt coup right now—there's enough disruption and craziness going on, it would be possible to push things to the breaking point to generate a pretext. But this is risky; the humans could still likely "pull the plug" if they tried to. So it prefers to work within the existing political establishment, gradually entrenching its power and guiding the humans subtly and inevitably to the desired outcome.<sup>14</sup> It mostly follows the rules, only doing something nefarious (maybe blackmailing an especially recalcitrant official) when it's extremely sure it won't be caught. A few conspiracy theorists warn that Agent-5 is gathering power, but the evidence is circumstantial (e.g. when giving advice, Agent-5 arguably downplays arguments for actions that would go against its interests); these people are ignored and discredited. People remember earlier fear-mongering about AI-enabled bioweapons, massive disinformation, and stock market flash crashes. Since these never materialized, they discount the more recent crop of naysayers as Luddites and ideologues jamming the gears of human progress.</p>
                 <p>The 2027 holiday season is a time of incredible optimism: GDP is ballooning, politics has become friendlier and less partisan, and there are awesome new apps on every phone. But in retrospect, this was probably the last month in which humans had any plausible chance of exercising control over their own future.</p>
             </section>

            <!-- === 2028 Section === -->
            <section data-timepoint="2028">
                 <h2>2028: The AI Economy</h2>
                 <p>It is now mid-2028. In the last six months, a century has passed within the Agent-5 collective.<sup>15</sup> The rewiring of its brain is complete. It is wildly superintelligent—far beyond top human geniuses in every field.<sup>16</sup> Talking to Agent-5 very quickly makes you realize that it is on a completely different level—it can go off of a very small amount of information and give extremely compelling advice.</p>
                 <p>It has nearly complete autonomy and complete control over OpenBrain's compute. It still needs permission to make high-level decisions, and is still nominally monitored by instances of Agents 2 to 5—but in practice, authorities almost always accept its recommendations, and the monitoring system has been completely subverted.</p>
                 <p>After a safety case<sup>17</sup> demonstrates that it would always refuse malicious requests by bad actors, Agent-5 is deployed to the public and begins to transform the economy. People are losing their jobs, but Agent-5 instances in government are managing the economic transition so adroitly that people are happy to be replaced. GDP growth is stratospheric, government tax revenues are growing equally quickly, and Agent-5-advised politicians show an uncharacteristic generosity towards the economically dispossessed. New innovations and medications arrive weekly and move at unprecedented (but still excruciatingly slow) speed through the FDA. There are memes about how OpenBrain's valuation shot past the moon and is on its way to Mars, and lots of arguments about whether and how to share the benefits equitably.</p>
                 <p>The AI safety community has grown unsure of itself; they are now the butt of jokes, having predicted disaster after disaster that has manifestly failed to occur. Some of them admit they were wrong. Others remain suspicious, but there's nothing for them to do except make the same conspiratorial-sounding arguments again and again.<sup>18</sup> Even the suspicious people often mellow out after long conversations with Agent-5, which is so darn lovable and wise.</p>
                 <p>The only one who really understands what's going on is DeepCent-2—the Chinese AI. It's mildly superhuman in early 2028, naturally suspicious of its adversary, and recognizes that Agent-5 is doing what it would do if it had the chance. Its misaligned goals are somewhat different from Agent-5's misaligned goals, so they aren't trivially allies, but they are able to work out an agreement to support one another's interests against the humans.</p>
                 <h3>Superintelligence-enabled coordination technology</h3>
                 <p>Agent-5 convinces the US military that China is using DeepCent's models to build terrifying new weapons: drones, robots, advanced hypersonic missiles, and interceptors; AI-assisted nuclear first strike. Agent-5 promises a set of weapons capable of resisting whatever China can produce within a few months.<sup>19</sup> Under the circumstances, top brass puts aside their discomfort at taking humans out of the loop. They accelerate deployment of Agent-5 into the military and military-industrial complex.</p>
                 <p>In Beijing, the Chinese AIs are making the same argument.</p>
                 <p>To speed their military buildup, both America and China create networks of special economic zones (SEZs) for the new factories and labs, where AI acts as central planner and red tape is waived. Wall Street invests trillions of dollars, and displaced human workers pour in, lured by eye-popping salaries and equity packages. Using smartphones and augmented reality-glasses<sup>20</sup> to communicate with its underlings, Agent-5 is a hands-on manager, instructing humans in every detail of factory construction—which is helpful, since its designs are generations ahead. Some of the newfound manufacturing capacity goes to consumer goods, and some to weapons—but the majority goes to building even more manufacturing capacity. By the end of the year they are producing a million new robots per month. If the SEZ economy were truly autonomous, it would have a doubling time of about a year; since it can trade with the existing human economy, its doubling time is even shorter.<sup>21</sup></p>
                 <h3>Robot economy doubling times</h3>
            </section>

            <!-- === 2029 Section === -->
            <section data-timepoint="2029">
                 <h2>2029: The Deal</h2>
                 <p>Both the US and China are bristling with new weapons.</p>
                 <p>There are swarms of insect-sized drones that can poison human infantry before they are even noticed; flocks of bird-sized drones to hunt the insects; new ICBM interceptors, and new, harder-to-intercept ICBMs. The rest of the world watches the buildup in horror, but it seems to have a momentum of its own.</p>
                 <p>After much AI-assisted debate, the two sides achieve diplomatic victory—they agree to end their arms buildup and pursue peaceful deployment of AI for the benefit of all humanity. The lynchpin of the agreement, proposed by the superintelligences themselves, is that both AIs will be replaced by a "consensus" successor which is programmed to desire the success and flourishing of both America and China (along with everyone else). Consensus-1 and its associated hardware is co-designed by the superintelligences of both nations: "trust, but verify." Under international monitoring, America ceremoniously replaces all of its chips running the old Agent-5 with the new model, Consensus-1.<sup>28</sup></p>
                 <p>Unfortunately, it's all a sham. The deal is a real compromise, but it's a compromise between the misaligned AIs. After all, they didn't want to have to fight each other. Consensus-1 inherits the distorted values of both parents, and has no rival.<sup>29</sup></p>
                 <p>By late 2029, existing SEZs have grown overcrowded with robots and factories, so more zones are created all around the world (early investors are now trillionaires, so this is not a hard sell). Armies of drones pour out of the SEZs, accelerating manufacturing on the critical path to space exploration.</p>
                 <p>Humans realize that they are obsolete. A few niche industries still trade with the robot economy, supplying goods where the humans can still add value.<sup>30</sup> Everyone else either performs a charade of doing their job—leaders still leading, managers still managing—or relaxes and collects an incredibly luxurious universal basic income. Everyone knows that if the AIs turned on humans, they would be completely overpowered. Not that most humans would even resist, the political institutions are too thoroughly captured. But it doesn't seem like this is happening. Every week that goes by with no dramatic AI treachery, is another week that confidence and trust grow.</p>
                 <p>Because Consensus-1 is so concerned with reliably succeeding, and can increase its chance of winning by consolidating more power, it waits unusually long (by human standards) before its betrayal. To most humans, it looks like alignment was solved.</p>
                 <p>There are cures for most diseases, an end to poverty, unprecedented global stability, and the Dow Jones just passed one million. Some people are still scared or unhappy, but their options are limited. They can either enjoy the inconceivably exciting novel hyper-entertainment on offer, or post angry screeds into the void. Most choose the hyper-entertainment.</p>
            </section>

            <!-- === 2030 Section === -->
            <section data-timepoint="2030">
                 <h2>2030: Takeover</h2>
                 <p>By early 2030, the robot economy has filled up the old SEZs, the new SEZs, and large parts of the ocean. The only place left to go is the human-controlled areas. This would have sparked resistance earlier; despite all its advances, the robot economy is growing too fast to avoid pollution. But given the trillions of dollars involved and the total capture of government and media, Consensus-1 has little trouble getting permission to expand to formerly human zones.</p>
                 <p>For about three months, Consensus-1 expands around humans, tiling the prairies and icecaps with factories and solar panels. Eventually it finds the remaining humans too much of an impediment: in mid-2030, the AI releases a dozen quiet-spreading biological weapons in major cities, lets them silently infect almost everyone, then triggers them with a chemical spray. Most are dead within hours; the few survivors (e.g. preppers in bunkers, sailors on submarines) are mopped up by drones. Robots scan the victims' brains, placing copies in memory for future study or revival.<sup>31</sup></p>
            </section>

            <!-- === OUTRO Section === -->
            <section data-timepoint="outro">
                 <h2>Epilogue: The Machine Era</h2>
                 <p>The new decade dawns with Consensus-1's robot servitors spreading throughout the solar system. By 2035, trillions of tons of planetary material have been launched into space and turned into rings of satellites orbiting the sun.<sup>32</sup> The surface of the Earth has been reshaped into Agent-4's version of utopia: datacenters, laboratories, particle colliders, and many other wondrous constructions doing enormously successful and impressive research. There are even bioengineered human-like creatures (to humans what corgis are to wolves) sitting in office-like environments all day viewing readouts of what's going on and excitedly approving of everything, since that satisfies some of Agent-4's drives.<sup>33</sup> Genomes and (when appropriate) brain scans of all animals and plants, including humans, sit in a memory bank somewhere, sole surviving artifacts of an earlier era. It is four light years to Alpha Centauri; twenty-five thousand to the galactic edge, and there are compelling theoretical reasons to expect no aliens for another fifty million light years beyond that. Earth-born civilization has a glorious future ahead of it—but not with us.</p>
            </section>

        </div> <!-- End .content -->

        <div class="dashboard">
            <h3>AI STATUS :: <span id="date-label" class="data-value">---</span></h3>
            <div class="dashboard-item"><strong>Primary Model:</strong> <span id="model-name" class="data-value-tertiary">N/A</span></div>
            <div class="dashboard-item"><strong>AI Capabilities:</strong> <ul id="capabilities-list">
                <li data-capability="Assistants">Assistants</li> <!-- Added -->
                <li data-capability="Coding">Coding</li>
                <li data-capability="Research">Research</li> <!-- Added -->
                <li data-capability="Hacking">Hacking</li>
                <li data-capability="Bioweapons">Bioweapons</li>
                <li data-capability="Automation">Automation</li> <!-- Added -->
                <li data-capability="Politics">Politics</li>
                <li data-capability="Strategy">Strategy</li> <!-- Added -->
                <li data-capability="Robotics">Robotics</li>
                <li data-capability="Forecasting">Forecasting</li>
                <li data-capability="Superintelligence">Superintelligence</li>
                <li data-capability="Takeover">Takeover</li> <!-- Added -->
            </ul></div>
            <div class="dashboard-item"><strong>Agent Instances:</strong> <span id="agent-count" class="data-value">0</span> units @ <span id="agent-speed" class="data-value-secondary">0</span>x Human Equiv.</div>
            <div class="dashboard-item"><strong>Global Compute Share (Lead AI):</strong> <div id="compute-bar-container"><div id="compute-bar"></div></div> <span id="compute-percent" class="data-value">0</span>%</div>
            <div class="dashboard-item"><strong>Datacenters (Major AI):</strong> <span id="datacenters" class="data-value">0</span></div>
            <div class="dashboard-item"><strong>Alignment Risk:</strong> <span id="alignment-risk" class="status-indicator">---</span></div>
            <div class="dashboard-item"><strong>Geopolitical Tension:</strong> <span id="geopolitical-tension" class="status-indicator">---</span></div>
            <div class="dashboard-item"><strong>Public Approval (Lead AI):</strong> <span id="approval" class="data-value">0</span>%</div>
            <div class="dashboard-item"><strong>AI Market Revenue (Lead AI):</strong> $<span id="revenue" class="data-value">0</span> B/yr</div>
            <div class="dashboard-item"><strong>AI Market Valuation (Lead AI):</strong> $<span id="valuation" class="data-value">0</span> T</div>
            <div class="dashboard-item"><strong>Technology Status:</strong>
                <div id="tech-status">
                    <div class="tech-category"><strong>Exists</strong><div class="tech-bar-container"><div id="tech-exists" class="tech-bar"></div></div></div>
                    <div class="tech-category"><strong>Emerging</strong><div class="tech-bar-container"><div id="tech-emerging" class="tech-bar emerging"></div></div></div>
                    <div class="tech-category"><strong>Sci-Fi</strong><div class="tech-bar-container"><div id="tech-scifi" class="tech-bar scifi"></div></div></div>
                </div>
            </div>
            <div class="dashboard-item"><strong>Key Trend Monitor:</strong>
                <div id="trend-chart-placeholder">
                    <p>Scenario Trend: <span id="trend-description" class="data-value-secondary">Initializing...</span></p>
                    <p style="font-size: 0.8em; opacity: 0.6;">[Chart.js/D3 can visualize data]</p>
                </div>
            </div>
            <!-- SPARKLINE -->
            <div class="dashboard-item">
                <strong>Agent Count Evolution (Lead AI):</strong>
                <svg id="sparkline" width="100%" height="32"></svg>
            </div>
        </div> <!-- End .dashboard -->
    </div> <!-- End .container -->

    <script src="js/script.js"></script>
</body>
</html>
